% @article{Beck2016Visual,
%   abstract = {Bibiographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.},
%   author = {Beck, Fabian and Koch, Sebastian and Weiskopf, Daniel},
%   doi = {10.1109/TVCG.2015.2467757},
%   journal = {IEEE Transactions on Visualization and Computer Graphics},
%   keywords = {type:system, visual_analytics, sparklines, information_retrieval, clustering, literature_browser},
%   number = {01},
%   publisher = {IEEE},
%   volume = {22},
%   series = {TVCG},
%   title = {Visual Analysis and Dissemination of Scientific Literature Collections with {SurVis}},
%   url = {http://www.visus.uni-stuttgart.de/uploads/tx_vispublications/vast15-survis.pdf},
%   year = {2016}
% }
@misc{li2025surveystateartlarge,
      title={A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges}, 
      author={Zongxia Li and Xiyang Wu and Hongyang Du and Fuxiao Liu and Huy Nghiem and Guangyao Shi},
      doi = {10.48550/arXiv.2501.02189},
      keywords = {type:Survey, Vision-Language Models, Multimodal Learning, Large Language Models, Visual Reasoning, Multimodal Alignment, Benchmarks, Evaluation Metrics, Hallucination, Reinforcement Learning from Human Feedback, Model Safety, Fairness, Self-Supervised Learning},
      year={2025},
      eprint={2501.02189},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.02189}, 
}
@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      doi = {10.48550/arXiv.2010.11929},
      keywords = {type:Method, Vision Transformer, Visual Task Adaptation Benchmark, ViT Models, ViT-L/16, Masked Patch Prediction, Patch Embeddings, ViT-B/16 Model, Attention Distance, ImageNet-21k, ViT-H/14},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}
@misc{bordes2024introductionvisionlanguagemodeling,
      title={An Introduction to Vision-Language Modeling}, 
      author={Florian Bordes and Richard Yuanzhe Pang and Anurag Ajay and Alexander C. Li and Adrien Bardes and Suzanne Petryk and Oscar Ma√±as and Zhiqiu Lin and Anas Mahmoud and Bargav Jayaraman and Mark Ibrahim and Melissa Hall and Yunyang Xiong and Jonathan Lebensold and Candace Ross and Srihari Jayakumar and Chuan Guo and Diane Bouchacourt and Haider Al-Tahan and Karthik Padthe and Vasu Sharma and Hu Xu and Xiaoqing Ellen Tan and Megan Richards and Samuel Lavoie and Pietro Astolfi and Reyhane Askari Hemmat and Jun Chen and Kushal Tirumala and Rim Assouel and Mazda Moayeri and Arjang Talattof and Kamalika Chaudhuri and Zechun Liu and Xilun Chen and Quentin Garrido and Karen Ullrich and Aishwarya Agrawal and Kate Saenko and Asli Celikyilmaz and Vikas Chandra},
      doi = {10.48550/arXiv.2405.17247},
      keywords = {type:Survey, Vision-Language Models (VLMs), Large Language Models (LLMs), Contrastive Learning, Generative Models, Pretrained Models, Image Captioning, Visual Question Answering (VQA), Masking Strategies, Multimodal Learning, Grounding and Alignment, Video Understanding, Data Curation, Model Evaluation, Zero-shot Learning, Cross-modal Learning, Multimodal Fusion},
      year={2024},
      eprint={2405.17247},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17247}, 
}
@misc{li2023blip2bootstrappinglanguageimagepretraining,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      doi = {10.48550/arXiv.2301.12597},
      keywords = {type:Method, BLIP-2, Frozen Image Encoders, Querying Transformer, Q-Former, Visual Conversation, EVA-CLIP, ViT-g/14, Perceiver Resampler, Image Encoders, Zero-shot Image-to-text Generation},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2301.12597}, 
}
@misc{alayrac2022flamingovisuallanguagemodel,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      doi = {10.48550/arXiv.2204.14198},
      keywords = {type:Method, Flamingo, Flamingo Models, Perceiver Resampler, Visual Language Model, Flamingo-80B, Interleaved Image, Outside Knowledge VQA, Multimodal Prompts, Vision Encoder, Interleaved Datasets},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.14198}, 
}
@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      doi = {10.48550/arXiv.2103.00020},
      keywords = {type:Method, Frozen Language Models, Interleaved Image, Vision Encoder, Visual Prefix, Frozen, Multimodal Few-shot Learning, Captioning Data, Outside Knowledge VQA, Few-shot Learning, Multimodal},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}
@misc{ying2024mmtbenchcomprehensivemultimodalbenchmark,
      title={MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI}, 
      author={Kaining Ying and Fanqing Meng and Jin Wang and Zhiqian Li and Han Lin and Yue Yang and Hao Zhang and Wenbo Zhang and Yuqi Lin and Shuo Liu and Jiayi Lei and Quanfeng Lu and Runjian Chen and Peng Xu and Renrui Zhang and Haozhe Zhang and Peng Gao and Yali Wang and Yu Qiao and Ping Luo and Kaipeng Zhang and Wenqi Shao},
      doi = {10.48550/arXiv.2404.16006},
      keywords = {type:Dataset, Multimodal Benchmark, Large Vision-Language Models (LVLMs), Multitask AGI, Visual Question Answering, Taskonomy, Instruction Tuning, Visual Reasoning, Multimodal Understanding, Evaluation Benchmark, MMT-Bench},
      year={2024},
      eprint={2404.16006},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.16006}, 
}
@misc{tsimpoukelli2021multimodalfewshotlearningfrozen,
      title={Multimodal Few-Shot Learning with Frozen Language Models}, 
      author={Maria Tsimpoukelli and Jacob Menick and Serkan Cabi and S. M. Ali Eslami and Oriol Vinyals and Felix Hill},
      doi = {10.48550/arXiv.2106.13884},
      keywords = {type:Method, Frozen Language Models, Interleaved Image, Vision Encoder, Visual Prefix, Frozen, Multimodal Few-shot Learning, Captioning Data, Outside Knowledge VQA, Few-shot Learning, Multimodal},
      year={2021},
      eprint={2106.13884},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2106.13884}, 
}
@misc{bai2025qwen25vltechnicalreport,
      title={Qwen2.5-VL Technical Report}, 
      author={Shuai Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Sibo Song and Kai Dang and Peng Wang and Shijie Wang and Jun Tang and Humen Zhong and Yuanzhi Zhu and Mingkun Yang and Zhaohai Li and Jianqiang Wan and Pengfei Wang and Wei Ding and Zheren Fu and Yiheng Xu and Jiabo Ye and Xi Zhang and Tianbao Xie and Zesen Cheng and Hang Zhang and Zhibo Yang and Haiyang Xu and Junyang Lin},
      doi = {10.48550/arXiv.2502.13923},
      keywords = {type:Method, Qwen2.5-VL, Vision-Language Model, Multimodal Transformer, Dynamic Resolution, Absolute Time Encoding, Window Attention, Visual Grounding, Document Parsing, Long-Video Understanding, MRoPE, Visual Encoder, Instruction Tuning, Agent Interaction, OCR, Point-based Grounding, Video Temporal Localization, Multimodal Alignment},
      year={2025},
      eprint={2502.13923},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.13923}, 
}
@misc{jia2021scalingvisualvisionlanguagerepresentation,
      title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}, 
      author={Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},
      doi = {10.48550/arXiv.2102.05918},
      keywords = {type:Method, Contrastive Language Image Pre Training, Conceptual Captions, Analog Layout , Intelligently Generate From Netlist, Flickr30K, Vision Language, Image-text Retrieval Tasks, Text Tower, Text-to-image Retrieval, CC3M, Vision-language Representations},
      year={2021},
      eprint={2102.05918},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.05918}, 
}
@misc{zhang2024visionlanguagemodelsvisiontasks,
      title={Vision-Language Models for Vision Tasks: A Survey}, 
      author={Jingyi Zhang and Jiaxing Huang and Sheng Jin and Shijian Lu},
      doi = {10.48550/arXiv.2304.00685},
      keywords = {type:Survey, Vision-language Correlation, Visual Recognition, Vision-Language Models, Systematic Review, Pretraining Methods, Zero-shot Prediction, Deep Neural Networks, Visual Language Model, Pre-training Objectives},
      year={2024},
      eprint={2304.00685},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.00685}, 
}
@misc{chou2025visionarena230krealworld,
      title={VisionArena: 230K Real World User-VLM Conversations with Preference Labels}, 
      author={Christopher Chou and Lisa Dunlap and Koki Mashita and Krishna Mandal and Trevor Darrell and Ion Stoica and Joseph E. Gonzalez and Wei-Lin Chiang},
      doi = {10.48550/arXiv.2412.08687},
      keywords = {type:Dataset, Vision-language models, Multimodal dialogue, User preference, Benchmark dataset, Instruction tuning, Model evaluation, Chatbot Arena, Real-world interactions, OCR, Image captioning, Visual reasoning, Human-AI interaction, Preference voting, VLM-as-a-judge, VisionArena-Bench},
      year={2025},
      eprint={2412.08687},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.08687}, 
}
@misc{agrawal2016vqavisualquestionanswering,
      title={VQA: Visual Question Answering}, 
      author={Aishwarya Agrawal and Jiasen Lu and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Dhruv Batra and Devi Parikh},
      doi = {10.48550/arXiv.1505.00468},
      keywords = {type:Dataset, Visual Question Answering, Visual Question Answering, Abstract Scenes, Visual Questions, Image Captioning, VQA Dataset, Yes/no, Question Types, Natural Language Questions, Visual Common Sense},
      year={2016},
      eprint={1505.00468},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1505.00468}, 
}
@misc{yarom2023readimprovingtextimagealignment,
      title={What You See is What You Read? Improving Text-Image Alignment Evaluation}, 
      author={Michal Yarom and Yonatan Bitton and Soravit Changpinyo and Roee Aharoni and Jonathan Herzig and Oran Lang and Eran Ofek and Idan Szpektor},
      doi = {10.48550/arXiv.2305.10400},
      keywords = {type:Method, SeeTRUE, TIFA, Complex Compositions, Pipeline, Vision-Language Models, Image-to-text, Human Judgements, Text-to-Image Generation, Unnatural Images, Question Generation},
      year={2023},
      eprint={2305.10400},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.10400}, 
}