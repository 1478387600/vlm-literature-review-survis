const generatedBibEntries = {
    "agrawal2016vqavisualquestionanswering": {
        "archiveprefix": "arXiv",
        "author": "Aishwarya Agrawal and Jiasen Lu and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Dhruv Batra and Devi Parikh",
        "doi": "10.48550/arXiv.1505.00468",
        "eprint": "1505.00468",
        "keywords": "type:Dataset, Visual Question Answering, Visual Question Answering, Abstract Scenes, Visual Questions, Image Captioning, VQA Dataset, Yes/no, Question Types, Natural Language Questions, Visual Common Sense",
        "primaryclass": "cs.CL",
        "title": "VQA: Visual Question Answering",
        "type": "misc",
        "url": "https://arxiv.org/abs/1505.00468",
        "year": "2016"
    },
    "alayrac2022flamingovisuallanguagemodel": {
        "archiveprefix": "arXiv",
        "author": "Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan",
        "doi": "10.48550/arXiv.2204.14198",
        "eprint": "2204.14198",
        "keywords": "type:Method, Flamingo, Flamingo Models, Perceiver Resampler, Visual Language Model, Flamingo-80B, Interleaved Image, Outside Knowledge VQA, Multimodal Prompts, Vision Encoder, Interleaved Datasets",
        "primaryclass": "cs.CV",
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
        "type": "misc",
        "url": "https://arxiv.org/abs/2204.14198",
        "year": "2022"
    },
    "bai2025qwen25vltechnicalreport": {
        "archiveprefix": "arXiv",
        "author": "Shuai Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Sibo Song and Kai Dang and Peng Wang and Shijie Wang and Jun Tang and Humen Zhong and Yuanzhi Zhu and Mingkun Yang and Zhaohai Li and Jianqiang Wan and Pengfei Wang and Wei Ding and Zheren Fu and Yiheng Xu and Jiabo Ye and Xi Zhang and Tianbao Xie and Zesen Cheng and Hang Zhang and Zhibo Yang and Haiyang Xu and Junyang Lin",
        "doi": "10.48550/arXiv.2502.13923",
        "eprint": "2502.13923",
        "keywords": "type:Method, Qwen2.5-VL, Vision-Language Model, Multimodal Transformer, Dynamic Resolution, Absolute Time Encoding, Window Attention, Visual Grounding, Document Parsing, Long-Video Understanding, MRoPE, Visual Encoder, Instruction Tuning, Agent Interaction, OCR, Point-based Grounding, Video Temporal Localization, Multimodal Alignment",
        "primaryclass": "cs.CV",
        "title": "Qwen2.5-VL Technical Report",
        "type": "misc",
        "url": "https://arxiv.org/abs/2502.13923",
        "year": "2025"
    },
    "bordes2024introductionvisionlanguagemodeling": {
        "archiveprefix": "arXiv",
        "author": "Florian Bordes and Richard Yuanzhe Pang and Anurag Ajay and Alexander C. Li and Adrien Bardes and Suzanne Petryk and Oscar Ma\u00f1as and Zhiqiu Lin and Anas Mahmoud and Bargav Jayaraman and Mark Ibrahim and Melissa Hall and Yunyang Xiong and Jonathan Lebensold and Candace Ross and Srihari Jayakumar and Chuan Guo and Diane Bouchacourt and Haider Al-Tahan and Karthik Padthe and Vasu Sharma and Hu Xu and Xiaoqing Ellen Tan and Megan Richards and Samuel Lavoie and Pietro Astolfi and Reyhane Askari Hemmat and Jun Chen and Kushal Tirumala and Rim Assouel and Mazda Moayeri and Arjang Talattof and Kamalika Chaudhuri and Zechun Liu and Xilun Chen and Quentin Garrido and Karen Ullrich and Aishwarya Agrawal and Kate Saenko and Asli Celikyilmaz and Vikas Chandra",
        "doi": "10.48550/arXiv.2405.17247",
        "eprint": "2405.17247",
        "keywords": "type:Survey, Vision-Language Models (VLMs), Large Language Models (LLMs), Contrastive Learning, Generative Models, Pretrained Models, Image Captioning, Visual Question Answering (VQA), Masking Strategies, Multimodal Learning, Grounding and Alignment, Video Understanding, Data Curation, Model Evaluation, Zero-shot Learning, Cross-modal Learning, Multimodal Fusion",
        "primaryclass": "cs.LG",
        "title": "An Introduction to Vision-Language Modeling",
        "type": "misc",
        "url": "https://arxiv.org/abs/2405.17247",
        "year": "2024"
    },
    "chou2025visionarena230krealworld": {
        "archiveprefix": "arXiv",
        "author": "Christopher Chou and Lisa Dunlap and Koki Mashita and Krishna Mandal and Trevor Darrell and Ion Stoica and Joseph E. Gonzalez and Wei-Lin Chiang",
        "doi": "10.48550/arXiv.2412.08687",
        "eprint": "2412.08687",
        "keywords": "type:Dataset, Vision-language models, Multimodal dialogue, User preference, Benchmark dataset, Instruction tuning, Model evaluation, Chatbot Arena, Real-world interactions, OCR, Image captioning, Visual reasoning, Human-AI interaction, Preference voting, VLM-as-a-judge, VisionArena-Bench",
        "primaryclass": "cs.CV",
        "title": "VisionArena: 230K Real World User-VLM Conversations with Preference Labels",
        "type": "misc",
        "url": "https://arxiv.org/abs/2412.08687",
        "year": "2025"
    },
    "dosovitskiy2021imageworth16x16words": {
        "archiveprefix": "arXiv",
        "author": "Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby",
        "doi": "10.48550/arXiv.2010.11929",
        "eprint": "2010.11929",
        "keywords": "type:Method, Vision Transformer, Visual Task Adaptation Benchmark, ViT Models, ViT-L/16, Masked Patch Prediction, Patch Embeddings, ViT-B/16 Model, Attention Distance, ImageNet-21k, ViT-H/14",
        "primaryclass": "cs.CV",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "type": "misc",
        "url": "https://arxiv.org/abs/2010.11929",
        "year": "2021"
    },
    "jia2021scalingvisualvisionlanguagerepresentation": {
        "archiveprefix": "arXiv",
        "author": "Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig",
        "doi": "10.48550/arXiv.2102.05918",
        "eprint": "2102.05918",
        "keywords": "type:Method, Contrastive Language Image Pre Training, Conceptual Captions, Analog Layout , Intelligently Generate From Netlist, Flickr30K, Vision Language, Image-text Retrieval Tasks, Text Tower, Text-to-image Retrieval, CC3M, Vision-language Representations",
        "primaryclass": "cs.CV",
        "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
        "type": "misc",
        "url": "https://arxiv.org/abs/2102.05918",
        "year": "2021"
    },
    "li2023blip2bootstrappinglanguageimagepretraining": {
        "archiveprefix": "arXiv",
        "author": "Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi",
        "doi": "10.48550/arXiv.2301.12597",
        "eprint": "2301.12597",
        "keywords": "type:Method, BLIP-2, Frozen Image Encoders, Querying Transformer, Q-Former, Visual Conversation, EVA-CLIP, ViT-g/14, Perceiver Resampler, Image Encoders, Zero-shot Image-to-text Generation",
        "primaryclass": "cs.CV",
        "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
        "type": "misc",
        "url": "https://arxiv.org/abs/2301.12597",
        "year": "2023"
    },
    "li2025surveystateartlarge": {
        "archiveprefix": "arXiv",
        "author": "Zongxia Li and Xiyang Wu and Hongyang Du and Fuxiao Liu and Huy Nghiem and Guangyao Shi",
        "doi": "10.48550/arXiv.2501.02189",
        "eprint": "2501.02189",
        "keywords": "type:Survey, Vision-Language Models, Multimodal Learning, Large Language Models, Visual Reasoning, Multimodal Alignment, Benchmarks, Evaluation Metrics, Hallucination, Reinforcement Learning from Human Feedback, Model Safety, Fairness, Self-Supervised Learning",
        "primaryclass": "cs.CV",
        "title": "A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges",
        "type": "misc",
        "url": "https://arxiv.org/abs/2501.02189",
        "year": "2025"
    },
    "radford2021learningtransferablevisualmodels": {
        "archiveprefix": "arXiv",
        "author": "Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever",
        "doi": "10.48550/arXiv.2103.00020",
        "eprint": "2103.00020",
        "keywords": "type:Method, Frozen Language Models, Interleaved Image, Vision Encoder, Visual Prefix, Frozen, Multimodal Few-shot Learning, Captioning Data, Outside Knowledge VQA, Few-shot Learning, Multimodal",
        "primaryclass": "cs.CV",
        "title": "Learning Transferable Visual Models From Natural Language Supervision",
        "type": "misc",
        "url": "https://arxiv.org/abs/2103.00020",
        "year": "2021"
    },
    "tsimpoukelli2021multimodalfewshotlearningfrozen": {
        "archiveprefix": "arXiv",
        "author": "Maria Tsimpoukelli and Jacob Menick and Serkan Cabi and S. M. Ali Eslami and Oriol Vinyals and Felix Hill",
        "doi": "10.48550/arXiv.2106.13884",
        "eprint": "2106.13884",
        "keywords": "type:Method, Frozen Language Models, Interleaved Image, Vision Encoder, Visual Prefix, Frozen, Multimodal Few-shot Learning, Captioning Data, Outside Knowledge VQA, Few-shot Learning, Multimodal",
        "primaryclass": "cs.CV",
        "title": "Multimodal Few-Shot Learning with Frozen Language Models",
        "type": "misc",
        "url": "https://arxiv.org/abs/2106.13884",
        "year": "2021"
    },
    "yarom2023readimprovingtextimagealignment": {
        "archiveprefix": "arXiv",
        "author": "Michal Yarom and Yonatan Bitton and Soravit Changpinyo and Roee Aharoni and Jonathan Herzig and Oran Lang and Eran Ofek and Idan Szpektor",
        "doi": "10.48550/arXiv.2305.10400",
        "eprint": "2305.10400",
        "keywords": "type:Method, SeeTRUE, TIFA, Complex Compositions, Pipeline, Vision-Language Models, Image-to-text, Human Judgements, Text-to-Image Generation, Unnatural Images, Question Generation",
        "primaryclass": "cs.CL",
        "title": "What You See is What You Read? Improving Text-Image Alignment Evaluation",
        "type": "misc",
        "url": "https://arxiv.org/abs/2305.10400",
        "year": "2023"
    },
    "ying2024mmtbenchcomprehensivemultimodalbenchmark": {
        "archiveprefix": "arXiv",
        "author": "Kaining Ying and Fanqing Meng and Jin Wang and Zhiqian Li and Han Lin and Yue Yang and Hao Zhang and Wenbo Zhang and Yuqi Lin and Shuo Liu and Jiayi Lei and Quanfeng Lu and Runjian Chen and Peng Xu and Renrui Zhang and Haozhe Zhang and Peng Gao and Yali Wang and Yu Qiao and Ping Luo and Kaipeng Zhang and Wenqi Shao",
        "doi": "10.48550/arXiv.2404.16006",
        "eprint": "2404.16006",
        "keywords": "type:Dataset, Multimodal Benchmark, Large Vision-Language Models (LVLMs), Multitask AGI, Visual Question Answering, Taskonomy, Instruction Tuning, Visual Reasoning, Multimodal Understanding, Evaluation Benchmark, MMT-Bench",
        "primaryclass": "cs.CV",
        "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
        "type": "misc",
        "url": "https://arxiv.org/abs/2404.16006",
        "year": "2024"
    },
    "zhang2024visionlanguagemodelsvisiontasks": {
        "archiveprefix": "arXiv",
        "author": "Jingyi Zhang and Jiaxing Huang and Sheng Jin and Shijian Lu",
        "doi": "10.48550/arXiv.2304.00685",
        "eprint": "2304.00685",
        "keywords": "type:Survey, Vision-language Correlation, Visual Recognition, Vision-Language Models, Systematic Review, Pretraining Methods, Zero-shot Prediction, Deep Neural Networks, Visual Language Model, Pre-training Objectives",
        "primaryclass": "cs.CV",
        "title": "Vision-Language Models for Vision Tasks: A Survey",
        "type": "misc",
        "url": "https://arxiv.org/abs/2304.00685",
        "year": "2024"
    }
};